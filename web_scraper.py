#!/usr/bin/env python3
"""
web_scraper.py - ì›¹ ìŠ¤í¬ë˜í•‘ ìœ í‹¸ë¦¬í‹°

ì›¹ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
BeautifulSoupì™€ requestsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì£¼ì˜: ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ robots.txtì™€ ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ì„¸ìš”.
"""

import re
import json
import csv
import time
import argparse
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
from pathlib import Path

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
try:
    import requests
    from bs4 import BeautifulSoup
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False


@dataclass
class ScrapedLink:
    """ìŠ¤í¬ë˜í•‘ëœ ë§í¬ ì •ë³´"""
    text: str
    url: str
    is_external: bool = False


@dataclass  
class ScrapedImage:
    """ìŠ¤í¬ë˜í•‘ëœ ì´ë¯¸ì§€ ì •ë³´"""
    src: str
    alt: str = ""
    width: Optional[int] = None
    height: Optional[int] = None


@dataclass
class PageMetadata:
    """í˜ì´ì§€ ë©”íƒ€ë°ì´í„°"""
    title: str = ""
    description: str = ""
    keywords: List[str] = field(default_factory=list)
    og_title: str = ""
    og_description: str = ""
    og_image: str = ""
    canonical_url: str = ""


@dataclass
class ScrapedPage:
    """ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ì •ë³´"""
    url: str
    status_code: int
    metadata: PageMetadata
    text_content: str
    links: List[ScrapedLink]
    images: List[ScrapedImage]
    headings: Dict[str, List[str]]
    tables: List[List[List[str]]]


class WebScraper:
    """
    ì›¹ ìŠ¤í¬ë˜í•‘ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    
    ì›¹ í˜ì´ì§€ì—ì„œ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    
    DEFAULT_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }
    
    def __init__(self, timeout: int = 10, delay: float = 1.0):
        """
        Args:
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            delay: ì—°ì† ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError(
                "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "ì„¤ì¹˜: pip install requests beautifulsoup4"
            )
        
        self.timeout = timeout
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update(self.DEFAULT_HEADERS)
        self.last_request_time = 0
    
    def _wait_for_delay(self) -> None:
        """ì—°ì† ìš”ì²­ ê°„ ë”œë ˆì´ë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)
        self.last_request_time = time.time()
    
    def fetch(self, url: str) -> requests.Response:
        """URLì—ì„œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        self._wait_for_delay()
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        return response
    
    def get_soup(self, url: str) -> BeautifulSoup:
        """URLì—ì„œ BeautifulSoup ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        response = self.fetch(url)
        return BeautifulSoup(response.text, "html.parser")
    
    def extract_metadata(self, soup: BeautifulSoup) -> PageMetadata:
        """í˜ì´ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        metadata = PageMetadata()
        
        # ì œëª©
        title_tag = soup.find("title")
        if title_tag:
            metadata.title = title_tag.get_text(strip=True)
        
        # meta íƒœê·¸ë“¤
        for meta in soup.find_all("meta"):
            name = meta.get("name", "").lower()
            property_attr = meta.get("property", "").lower()
            content = meta.get("content", "")
            
            if name == "description":
                metadata.description = content
            elif name == "keywords":
                metadata.keywords = [k.strip() for k in content.split(",")]
            elif property_attr == "og:title":
                metadata.og_title = content
            elif property_attr == "og:description":
                metadata.og_description = content
            elif property_attr == "og:image":
                metadata.og_image = content
        
        # canonical URL
        canonical = soup.find("link", rel="canonical")
        if canonical:
            metadata.canonical_url = canonical.get("href", "")
        
        return metadata
    
    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[ScrapedLink]:
        """í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links = []
        base_domain = urlparse(base_url).netloc
        
        for a in soup.find_all("a", href=True):
            href = a["href"]
            
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            full_url = urljoin(base_url, href)
            
            # ë§í¬ ë„ë©”ì¸ í™•ì¸
            link_domain = urlparse(full_url).netloc
            is_external = link_domain != base_domain
            
            text = a.get_text(strip=True) or "[ì´ë¯¸ì§€/ì•„ì´ì½˜]"
            
            links.append(ScrapedLink(
                text=text[:100],  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                url=full_url,
                is_external=is_external
            ))
        
        return links
    
    def extract_images(self, soup: BeautifulSoup, base_url: str) -> List[ScrapedImage]:
        """í˜ì´ì§€ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        images = []
        
        for img in soup.find_all("img"):
            src = img.get("src", "")
            if not src:
                continue
            
            full_src = urljoin(base_url, src)
            
            # í¬ê¸° ì •ë³´
            width = img.get("width")
            height = img.get("height")
            
            images.append(ScrapedImage(
                src=full_src,
                alt=img.get("alt", ""),
                width=int(width) if width and width.isdigit() else None,
                height=int(height) if height and height.isdigit() else None
            ))
        
        return images
    
    def extract_headings(self, soup: BeautifulSoup) -> Dict[str, List[str]]:
        """h1~h6 ì œëª©ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        headings = {}
        
        for level in range(1, 7):
            tag_name = f"h{level}"
            found = soup.find_all(tag_name)
            if found:
                headings[tag_name] = [h.get_text(strip=True) for h in found]
        
        return headings
    
    def extract_tables(self, soup: BeautifulSoup) -> List[List[List[str]]]:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        tables = []
        
        for table in soup.find_all("table"):
            table_data = []
            
            for row in table.find_all("tr"):
                cells = row.find_all(["th", "td"])
                row_data = [cell.get_text(strip=True) for cell in cells]
                if row_data:
                    table_data.append(row_data)
            
            if table_data:
                tables.append(table_data)
        
        return tables
    
    def extract_text(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.decompose()
        
        text = soup.get_text(separator="\n", strip=True)
        
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
        text = re.sub(r"\n{3,}", "\n\n", text)
        
        return text
    
    def scrape(self, url: str) -> ScrapedPage:
        """URLì„ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ëª¨ë“  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        response = self.fetch(url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        return ScrapedPage(
            url=url,
            status_code=response.status_code,
            metadata=self.extract_metadata(soup),
            text_content=self.extract_text(BeautifulSoup(response.text, "html.parser")),
            links=self.extract_links(soup, url),
            images=self.extract_images(soup, url),
            headings=self.extract_headings(soup),
            tables=self.extract_tables(soup)
        )
    
    def find_elements(self, url: str, selector: str) -> List[str]:
        """CSS ì„ íƒìë¡œ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        soup = self.get_soup(url)
        elements = soup.select(selector)
        return [el.get_text(strip=True) for el in elements]
    
    def download_image(self, url: str, save_path: str) -> bool:
        """ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            response = self.session.get(url, timeout=self.timeout, stream=True)
            response.raise_for_status()
            
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
        except Exception:
            return False


def print_scraped_page(page: ScrapedPage, verbose: bool = False) -> None:
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 60)
    print(f"ğŸŒ ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {page.url}")
    print("=" * 60)
    
    print(f"\nğŸ“‹ ìƒíƒœ ì½”ë“œ: {page.status_code}")
    
    print("\nğŸ“‘ ë©”íƒ€ë°ì´í„°:")
    print("-" * 40)
    print(f"  ì œëª©: {page.metadata.title}")
    if page.metadata.description:
        print(f"  ì„¤ëª…: {page.metadata.description[:100]}...")
    if page.metadata.keywords:
        print(f"  í‚¤ì›Œë“œ: {', '.join(page.metadata.keywords[:5])}")
    if page.metadata.og_image:
        print(f"  OG ì´ë¯¸ì§€: {page.metadata.og_image}")
    
    print(f"\nğŸ“Š í†µê³„:")
    print("-" * 40)
    print(f"  ë§í¬ ìˆ˜: {len(page.links)} (ì™¸ë¶€: {sum(1 for l in page.links if l.is_external)})")
    print(f"  ì´ë¯¸ì§€ ìˆ˜: {len(page.images)}")
    print(f"  í…Œì´ë¸” ìˆ˜: {len(page.tables)}")
    print(f"  í…ìŠ¤íŠ¸ ê¸¸ì´: {len(page.text_content)} ì")
    
    if page.headings:
        print("\nğŸ“Œ ì œëª© êµ¬ì¡°:")
        print("-" * 40)
        for tag, texts in page.headings.items():
            for text in texts[:3]:  # ê° ë ˆë²¨ë‹¹ ìµœëŒ€ 3ê°œ
                indent = "  " * int(tag[1])
                print(f"{indent}{tag}: {text[:50]}")
    
    if verbose:
        if page.links:
            print("\nğŸ”— ë§í¬ (ì²˜ìŒ 10ê°œ):")
            print("-" * 40)
            for link in page.links[:10]:
                external_mark = " [ì™¸ë¶€]" if link.is_external else ""
                print(f"  â€¢ {link.text[:30]}: {link.url[:50]}{external_mark}")
        
        if page.images:
            print("\nğŸ–¼ï¸ ì´ë¯¸ì§€ (ì²˜ìŒ 5ê°œ):")
            print("-" * 40)
            for img in page.images[:5]:
                print(f"  â€¢ {img.alt[:30] or '(alt ì—†ìŒ)'}: {img.src[:50]}")
        
        if page.tables:
            print("\nğŸ“Š í…Œì´ë¸”:")
            print("-" * 40)
            for i, table in enumerate(page.tables[:2]):
                print(f"  í…Œì´ë¸” {i + 1}: {len(table)} í–‰")
                for row in table[:3]:
                    print(f"    {' | '.join(str(cell)[:15] for cell in row[:4])}")


def export_to_json(page: ScrapedPage, filepath: str) -> None:
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    data = {
        "url": page.url,
        "status_code": page.status_code,
        "metadata": {
            "title": page.metadata.title,
            "description": page.metadata.description,
            "keywords": page.metadata.keywords,
            "og_title": page.metadata.og_title,
            "og_description": page.metadata.og_description,
            "og_image": page.metadata.og_image,
            "canonical_url": page.metadata.canonical_url,
        },
        "links": [{"text": l.text, "url": l.url, "is_external": l.is_external} for l in page.links],
        "images": [{"src": i.src, "alt": i.alt} for i in page.images],
        "headings": page.headings,
        "tables": page.tables,
        "text_content": page.text_content,
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filepath}")


def export_links_to_csv(links: List[ScrapedLink], filepath: str) -> None:
    """ë§í¬ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(filepath, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["í…ìŠ¤íŠ¸", "URL", "ì™¸ë¶€ë§í¬"])
        for link in links:
            writer.writerow([link.text, link.url, "ì˜ˆ" if link.is_external else "ì•„ë‹ˆì˜¤"])
    
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filepath}")


def main():
    """ë©”ì¸ CLI í•¨ìˆ˜"""
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜: pip install requests beautifulsoup4")
        return
    
    parser = argparse.ArgumentParser(
        description="ğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ìœ í‹¸ë¦¬í‹°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python web_scraper.py https://example.com              # ê¸°ë³¸ ìŠ¤í¬ë˜í•‘
  python web_scraper.py https://example.com -v           # ìƒì„¸ ì¶œë ¥
  python web_scraper.py https://example.com --json out.json  # JSON ì €ì¥
  python web_scraper.py https://example.com --links      # ë§í¬ë§Œ ì¶”ì¶œ
  python web_scraper.py https://example.com -s "h1"      # CSS ì„ íƒìë¡œ ì¶”ì¶œ

ì£¼ì˜: ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ robots.txtì™€ ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ì„¸ìš”.
        """
    )
    
    parser.add_argument("url", help="ìŠ¤í¬ë˜í•‘í•  URL")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument("--json", type=str, metavar="FILE",
                        help="ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--links", action="store_true",
                        help="ë§í¬ë§Œ ì¶”ì¶œ")
    parser.add_argument("--images", action="store_true",
                        help="ì´ë¯¸ì§€ë§Œ ì¶”ì¶œ")
    parser.add_argument("--text", action="store_true",
                        help="í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ")
    parser.add_argument("-s", "--selector", type=str,
                        help="CSS ì„ íƒìë¡œ ìš”ì†Œ ì¶”ì¶œ")
    parser.add_argument("--csv", type=str, metavar="FILE",
                        help="ë§í¬ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--timeout", type=int, default=10,
                        help="ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 10)")
    
    args = parser.parse_args()
    
    try:
        scraper = WebScraper(timeout=args.timeout)
        
        # CSS ì„ íƒì ëª¨ë“œ
        if args.selector:
            print(f"\nğŸ” ì„ íƒì '{args.selector}'ë¡œ ê²€ìƒ‰ ì¤‘...")
            results = scraper.find_elements(args.url, args.selector)
            
            print(f"\nì°¾ì€ ìš”ì†Œ: {len(results)}ê°œ")
            print("-" * 40)
            for i, text in enumerate(results[:20], 1):
                print(f"  {i}. {text[:100]}")
            return
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        print(f"\nğŸ”„ ìŠ¤í¬ë˜í•‘ ì¤‘: {args.url}")
        page = scraper.scrape(args.url)
        
        # íŠ¹ì • ë°ì´í„°ë§Œ ì¶œë ¥
        if args.links:
            print(f"\nğŸ”— ë§í¬ ({len(page.links)}ê°œ):")
            print("-" * 40)
            for link in page.links:
                external = " [ì™¸ë¶€]" if link.is_external else ""
                print(f"  â€¢ {link.text}: {link.url}{external}")
            
            if args.csv:
                export_links_to_csv(page.links, args.csv)
            return
        
        if args.images:
            print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ ({len(page.images)}ê°œ):")
            print("-" * 40)
            for img in page.images:
                print(f"  â€¢ {img.alt or '(alt ì—†ìŒ)'}: {img.src}")
            return
        
        if args.text:
            print("\nğŸ“„ í…ìŠ¤íŠ¸ ì½˜í…ì¸ :")
            print("-" * 40)
            print(page.text_content[:5000])
            if len(page.text_content) > 5000:
                print(f"\n... (ì´ {len(page.text_content)} ì)")
            return
        
        # ì „ì²´ ê²°ê³¼ ì¶œë ¥
        print_scraped_page(page, verbose=args.verbose)
        
        # JSON ì €ì¥
        if args.json:
            export_to_json(page, args.json)
    
    except requests.exceptions.RequestException as e:
        print(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()

